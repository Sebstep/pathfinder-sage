# Limitations of language models

- **Knowledge limit**: Language models were trained with data up to a certain date. Current information must be passed in the prompt.
- **Biased training data**: Language models have been trained on a variety of sources that may contain biases and lead to biased answers.
- **Hallucinations**: Language models may confidently present false information, especially for queries beyond its knowledge boundary.

## When can language models be used?

- **What happens if the output is wrong?** If the use case requires absolute accuracy, as in diagnostics, an LLM may not be the safest choice.
- **How will you check the accuracy of the answer?** Only use ChatGPT for tasks that you could solve yourself given enough time.
- **What data are you entering?** For sensitive data (e.g. personal information or source code), you may need to obtain the necessary consent.
- **Is ownership of the answer required?** If the output is to be used commercially (e.g. music, literature), inadvertent infringements of existing copyrights are possible.

## Who owns the output?

In general, users can claim ownership of the answers as long as the terms of use and applicable laws are complied with.

- Non-unique responses cannot be claimed as they could also be generated for other users.
- For products or tools based on ChatGPT, it must be clear to users that the answers do not originate from a human being.
- Language models must not be used for legal infringements (e.g. copyright infringements). If content is similar to copyrighted material, this could lead to claims.
  - Example: If we ask a language model to write a song in the style of an artist, it could be legally unclear who owns the result - the user, the artist or the model developer. Such legal gray areas must be considered before use.
